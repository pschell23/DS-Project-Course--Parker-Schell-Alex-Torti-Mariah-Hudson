{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.metrics import accuracy_score, classification_reportt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 34886 entries, 0 to 34885\n",
      "Data columns (total 8 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   Release Year      34886 non-null  int64 \n",
      " 1   Title             34886 non-null  object\n",
      " 2   Origin/Ethnicity  34886 non-null  object\n",
      " 3   Director          34886 non-null  object\n",
      " 4   Cast              33464 non-null  object\n",
      " 5   Genre             34886 non-null  object\n",
      " 6   Wiki Page         34886 non-null  object\n",
      " 7   Plot              34886 non-null  object\n",
      "dtypes: int64(1), object(7)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "movie_data = pd.read_csv(r\"/Users/alexandratorti/DS PROJECT COURSE/Project 2/wiki_movie_plots_deduped.csv\")\n",
    "movie_data.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Data (Dropping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 17377 entries, 0 to 17376\n",
      "Data columns (total 5 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   Release Year      17377 non-null  int64 \n",
      " 1   Title             17377 non-null  object\n",
      " 2   Origin/Ethnicity  17377 non-null  object\n",
      " 3   Genre             17377 non-null  object\n",
      " 4   Plot              17377 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 814.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# Select only films that are American in origin\n",
    "us_movies = movie_data[movie_data['Origin/Ethnicity'] == 'American']\n",
    "\n",
    "# Drop unwanted columns (Wiki.Page, Director, Cast)\n",
    "us_movies = us_movies.drop(columns=['Wiki Page', 'Director', 'Cast'])\n",
    "\n",
    "# Print the summary of the resulting DataFrame\n",
    "us_movies.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Plot</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Title</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Kansas Saloon Smashers</th>\n",
       "      <td>A bartender is working at a saloon, serving dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Love by the Light of the Moon</th>\n",
       "      <td>The moon, painted with a smiling face hangs ov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Martyred Presidents</th>\n",
       "      <td>The film, just over a minute long, is composed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Terrible Teddy, the Grizzly King</th>\n",
       "      <td>Lasting just 61 seconds and consisting of two ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jack and the Beanstalk</th>\n",
       "      <td>The earliest known adaptation of the classic f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                               Plot\n",
       "Title                                                                              \n",
       "Kansas Saloon Smashers            A bartender is working at a saloon, serving dr...\n",
       "Love by the Light of the Moon     The moon, painted with a smiling face hangs ov...\n",
       "The Martyred Presidents           The film, just over a minute long, is composed...\n",
       "Terrible Teddy, the Grizzly King  Lasting just 61 seconds and consisting of two ...\n",
       "Jack and the Beanstalk            The earliest known adaptation of the classic f..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch Title and Plot columns\n",
    "finaldata = us_movies[[\"Title\", \"Plot\"]]\n",
    "# Setting the movie title as index\n",
    "finaldata = finaldata.set_index('Title')\n",
    "finaldata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Project two, Movie recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guiding Question\n",
    "- How do we create a machine learning model that offers recommendations based on movie plot? \n",
    "\n",
    "## Data Background\n",
    "- Data Dimensions \n",
    "- ~35,000 movies (rows) x 8 columns \n",
    "\n",
    "### Columns in the data\n",
    "- Year\n",
    "- Title\n",
    "- Origin/Ethnicity (dropping everything that’s not American) \n",
    "- Director (dropping)\n",
    "- Cast (dropping)\n",
    "- Genre\n",
    "- Wiki page (dropping) \n",
    "- Plot \n",
    "\n",
    "### What our cleaned data set looks like (before LDA pre-processing)\n",
    "- 5 x ~17,500\n",
    "- Year, Title, Origin = American, Genre, Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Background \n",
    "- Because our recommender will be based of text recognition and synopsis, we must have a way to identify trends within each text piece.\n",
    "- LDA (Latent Dirichlet Allocation) is a topic modeling technique used for extracting topics from text documents. It finds topics that the “text” belongs to on the basis of the specific words it contains. It then groups similar topics allowing to map probability distribution over latent topics\n",
    "\n",
    "### Term Frequency-Inverse Document Frequency\n",
    "- Count each words frequency thus converting the text format to a numerical one\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing LDA and Tokenizer Specific Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using nltk and spaCy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) dividing text into sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize\n",
    "\n",
    "# def sent_to_words(sentences):\n",
    "#     for sentence in sentences:\n",
    "#             sentence = sentence.lower()(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "# data_words = list(sent_to_words(data))\n",
    "# #print(data_words[:1])\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "#we will not need to process data to take out unimportant words\n",
    "\n",
    "# Download stopwords dataset\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [word.lower() for word in tokens if word.isalnum() and word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "finaldata['Processed_Plot'] = finaldata['Plot'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Plot</th>\n",
       "      <th>Processed_Plot</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Title</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Kansas Saloon Smashers</th>\n",
       "      <td>A bartender is working at a saloon, serving dr...</td>\n",
       "      <td>bartender working saloon serving drinks custom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Love by the Light of the Moon</th>\n",
       "      <td>The moon, painted with a smiling face hangs ov...</td>\n",
       "      <td>moon painted smiling face hangs park night you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Martyred Presidents</th>\n",
       "      <td>The film, just over a minute long, is composed...</td>\n",
       "      <td>film minute long composed two shots first girl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Terrible Teddy, the Grizzly King</th>\n",
       "      <td>Lasting just 61 seconds and consisting of two ...</td>\n",
       "      <td>lasting 61 seconds consisting two shots first ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jack and the Beanstalk</th>\n",
       "      <td>The earliest known adaptation of the classic f...</td>\n",
       "      <td>earliest known adaptation classic fairytale fi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                               Plot  \\\n",
       "Title                                                                                 \n",
       "Kansas Saloon Smashers            A bartender is working at a saloon, serving dr...   \n",
       "Love by the Light of the Moon     The moon, painted with a smiling face hangs ov...   \n",
       "The Martyred Presidents           The film, just over a minute long, is composed...   \n",
       "Terrible Teddy, the Grizzly King  Lasting just 61 seconds and consisting of two ...   \n",
       "Jack and the Beanstalk            The earliest known adaptation of the classic f...   \n",
       "\n",
       "                                                                     Processed_Plot  \n",
       "Title                                                                                \n",
       "Kansas Saloon Smashers            bartender working saloon serving drinks custom...  \n",
       "Love by the Light of the Moon     moon painted smiling face hangs park night you...  \n",
       "The Martyred Presidents           film minute long composed two shots first girl...  \n",
       "Terrible Teddy, the Grizzly King  lasting 61 seconds consisting two shots first ...  \n",
       "Jack and the Beanstalk            earliest known adaptation classic fairytale fi...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finaldata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['bartender', 'is', 'working', 'at', 'saloon', 'serving', 'drinks', 'to', 'customers', 'after', 'he', 'fills', 'stereotypically', 'irish', 'man', 'bucket', 'with', 'beer', 'carrie', 'nation', 'and', 'her', 'followers', 'burst', 'inside', 'they', 'assault', 'the', 'irish', 'man', 'pulling', 'his', 'hat', 'over', 'his', 'eyes', 'and', 'then', 'dumping', 'the', 'beer', 'over', 'his', 'head', 'the', 'group', 'then', 'begin', 'wrecking', 'the', 'bar', 'smashing', 'the', 'fixtures', 'mirrors', 'and', 'breaking', 'the', 'cash', 'register', 'the', 'bartender', 'then', 'sprays', 'seltzer', 'water', 'in', 'nation', 'face', 'before', 'group', 'of', 'policemen', 'appear', 'and', 'order', 'everybody', 'to', 'leave']]\n"
     ]
    }
   ],
   "source": [
    "# Extract the 'Plot' column values from the DataFrame and convert to a list\n",
    "finaldata = finaldata.Plot.values.tolist()\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "data_words = list(sent_to_words(finalda # Convert the list of sentences to a list of tokenized wordsta))\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']): #'NOUN', 'ADJ', 'VERB', 'ADV'\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".joinent)) \n",
    "        texts_out.append(\" \".join([token.lemma_ if        # Extract lemmatized forms, excluding pronouns, and considering specified POS tags token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n",
    "    return texts_out # Initialize an empty list to store lemmatized te #returns the list of lemmatized textsxts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E941] Can't find model 'en'. It looks like you're trying to load a model from a shortcut, which is obsolete as of spaCy v3.0. To load the model, use its full name instead:\n\nnlp = spacy.load(\"en_core_web_sm\")\n\nFor more details on the available models, see the models directory: https://spacy.io/models. If you want to create a blank model, use spacy.blank: nlp = spacy.blank(\"en\")",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39men\u001b[39;49m\u001b[39m'\u001b[39;49m, disable\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mparser\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mner\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m      2\u001b[0m \u001b[39m# Do lemmatization keeping only Noun, Adj, Verb, Adverb\u001b[39;00m\n\u001b[1;32m      3\u001b[0m data_lemmatized \u001b[39m=\u001b[39m lemmatization(finaldata[\u001b[39m\"\u001b[39m\u001b[39mProcessed_Plot\u001b[39m\u001b[39m\"\u001b[39m], allowed_postags\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mNOUN\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mVERB\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39m#select noun and verb\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/spacy/__init__.py:54\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\n\u001b[1;32m     31\u001b[0m     name: Union[\u001b[39mstr\u001b[39m, Path],\n\u001b[1;32m     32\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     config: Union[Dict[\u001b[39mstr\u001b[39m, Any], Config] \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     38\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Language:\n\u001b[1;32m     39\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[39m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m util\u001b[39m.\u001b[39;49mload_model(\n\u001b[1;32m     55\u001b[0m         name,\n\u001b[1;32m     56\u001b[0m         vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[1;32m     57\u001b[0m         disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[1;32m     58\u001b[0m         enable\u001b[39m=\u001b[39;49menable,\n\u001b[1;32m     59\u001b[0m         exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[1;32m     60\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m     61\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/spacy/util.py:448\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[39mreturn\u001b[39;00m load_model_from_path(name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m--> 448\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE941\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname, full\u001b[39m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[39m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE050\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname))\n",
      "\u001b[0;31mOSError\u001b[0m: [E941] Can't find model 'en'. It looks like you're trying to load a model from a shortcut, which is obsolete as of spaCy v3.0. To load the model, use its full name instead:\n\nnlp = spacy.load(\"en_core_web_sm\")\n\nFor more details on the available models, see the models directory: https://spacy.io/models. If you want to create a blank model, use spacy.blank: nlp = spacy.blank(\"en\")"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (356471259.py, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[27], line 14\u001b[0;36m\u001b[0m\n\u001b[0;31m    data_vectorized = vectorizer.fit_transform(data_lemmatized)\u001b[0m\n\u001b[0m                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "## Document word matrix \n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=10,\n",
    "# minimum reqd occurences of a word \n",
    "                             stop_words='english',             \n",
    "# remove stop words\n",
    "                             lowercase=True,                   \n",
    "# convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  \n",
    "# num chars > 3\n",
    "                             # max_features=50000,             \n",
    "# max number of uniq words    )\n",
    "data_vectorized = vectorizer.fit_transform(data_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replace newlines with spaces\n",
    "\n",
    "#text = .replace(\"\\n\", \" \")\n",
    "\n",
    "#doc = nlp(finaldata['Processed_Plot'])\n",
    "#sentences = [sentence.text for sentence in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the spaCy engine\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Create empty lists to store results\n",
    "all_words = []\n",
    "all_pos = []\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in finaldata.iterrows():\n",
    "    # Read in the book text from the 'processed_plot' column\n",
    "    text = row['Processed_Plot']\n",
    "\n",
    "    # Replace newlines with spaces\n",
    "    #text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "    # Use the spaCy engine to process the text\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Get the list of tuples with words and parts of speech tags\n",
    "    words = [token.text for token in doc]\n",
    "    pos = [token.pos_ for token in doc]\n",
    "\n",
    "    # Append the results to the lists\n",
    "    all_words.append(words)\n",
    "    all_pos.append(pos)\n",
    "\n",
    "# Add the lists of words and parts of speech as new columns to the DataFrame\n",
    "finaldata['words'] = all_words\n",
    "finaldata['pos_tags'] = all_pos\n",
    "\n",
    "# Display the DataFrame\n",
    "print(finaldata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the NLTK Snowball Stemmer:\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Initialize stemmer with English:\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# Initialize a list with words to stem:\n",
    "words\n",
    "\n",
    "# Stem the words:\n",
    "\n",
    "for word in words:\n",
    "    stemmed_words = [stemmer.stem(words)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combine parts of speech tagging and lemmatization\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
